{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet : Detection d’Anomalies dans le Trafic Reseau avec LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data-20221207.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35250, 84)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \"\"\"\n",
    "    Prétraitement des Données\n",
    "\n",
    "    • Nettoyage : Imputer les valeurs manquantes.\n",
    "    • Encodage des variables catégorielles : Appliquer l’encodage par variables factices (dummy variables)\n",
    "      pour les données non numériques.\n",
    "    • Standardisation : Standardiser les données numériques pour améliorer les performances des algorithmes.\n",
    "    • Livrable : Code et documentation du pipeline de prétraitement des données.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Le DataFrame contenant les données à traiter.\n",
    "    target_column : str\n",
    "        La colonne cible pour le modèle.\n",
    "    exclude_columns : list\n",
    "        Les colonnes à exclure du prétraitement (par défaut, aucune colonne n'est exclue).\n",
    "    test_size : float\n",
    "        La proportion de l'ensemble de test (par défaut, 0.2).\n",
    "    random_state : int\n",
    "        Graine aléatoire pour la reproductibilité du découpage en train/test (par défaut, 0).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, target_column, exclude_columns=None, test_size=0.2, random_state=0):\n",
    "        \n",
    "        self.df = df\n",
    "        self.target_column = target_column\n",
    "        self.exclude_columns = exclude_columns if exclude_columns is not None else []\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "\n",
    "\n",
    "    def impute_outliers(self, colname): \n",
    "        q1 = np.percentile(self.df[colname], 25)\n",
    "        q3 = np.percentile(self.df[colname], 75) \n",
    "\n",
    "        lower_bound = q1 - 1.5*(q3 - q1)\n",
    "        upper_bound = q3 + 1.5*(q3 - q1)\n",
    "\n",
    "        self.df.loc[(self.df[colname] <= lower_bound), colname] = lower_bound\n",
    "        self.df.loc[(self.df[colname] >= upper_bound), colname] = upper_bound\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Impute les valeurs aberrantes pour toutes les colonnes numériques.\n",
    "        \"\"\"\n",
    "        for colname in self.df.select_dtypes('number').columns:\n",
    "            self.impute_outliers(colname)\n",
    "        \n",
    "\n",
    "    def split_data(self):\n",
    "        \"\"\"\n",
    "        Divise les données en ensembles d'entraînement et de test.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        X_train, X_test, y_train, y_test : tuple\n",
    "            Données divisées en ensembles d'entraînement et de test.\n",
    "        \"\"\"\n",
    "        y = self.df[self.target_column]\n",
    "        X = self.df.drop([self.target_column] + self.exclude_columns, axis='columns')\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                 X, \n",
    "                                 y, \n",
    "                                 test_size=self.test_size, \n",
    "                                 random_state=self.random_state\n",
    "                            )\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    \n",
    "\n",
    "    def create_pipeline(self, X_train):\n",
    "        \"\"\"\n",
    "        Crée un pipeline de prétraitement pour les données.\n",
    "\n",
    "        Étapes du pipeline :\n",
    "        - Imputation des valeurs manquantes.\n",
    "        - Standardisation des colonnes numériques.\n",
    "        - Encodage des variables catégorielles avec des variables factices.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_train : pd.DataFrame\n",
    "            Données d'entraînement pour déterminer les types de colonnes.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        full_pipeline : ColumnTransformer\n",
    "            Pipeline de transformations pour le prétraitement complet des données.\n",
    "        \"\"\"\n",
    "        num_cols = X_train.select_dtypes(include=['number']).columns\n",
    "        cat_cols = X_train.select_dtypes(include='object').columns\n",
    "        num_pipeline = make_pipeline(\n",
    "                    SimpleImputer(strategy='median'),\n",
    "                    StandardScaler(),\n",
    "                )\n",
    "        cat_pipeline = make_pipeline(\n",
    "                    SimpleImputer(strategy='most_frequent'),\n",
    "                    OneHotEncoder(handle_unknown='ignore', drop='first')\n",
    "                )\n",
    "        full_pipeline = ColumnTransformer([\n",
    "                            ('num', num_pipeline, num_cols),\n",
    "                            ('cat', cat_pipeline, cat_cols)\n",
    "                        ])\n",
    "        return full_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = DataPreprocessor(df, 'satisfaction',random_state=10)\n",
    "X_train, X_test, y_train, y_test = preprocessing.split_data()\n",
    "pipeline = preprocessing.create_pipeline(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transform = pipeline.fit_transform(X_train)\n",
    "X_test_transform = pipeline.transform(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
