{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet : Detection d’Anomalies dans le Trafic Reseau avec LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from joblib import Parallel, delayed\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import warnings\n",
    "\n",
    "# Ignorer tous les warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data-20221207.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans notre analyse pour detecter les anomalies, nous remarquons que les adresses IP, les ports et les protocoles n'ont pas une importance signifigative sur notre travail. Et, nous allons supprimer tous les variables qui sont tous de types objects.\n",
    "\n",
    "Pour continuer notre travail et faire du lightGBM, nous allons l'associer avec du IsolationForest pour avoir de bons resultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.drop(columns=df.select_dtypes(include=[\"object\"]).columns)\n",
    "\n",
    "# Détection d'anomalies avec Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "df_clean['anomaly_score'] = iso_forest.fit_predict(df_clean)\n",
    "\n",
    "# Création des labels artificiels (1 = anomalie, 0 = normal)\n",
    "df['Label'] = (df_clean['anomaly_score'] == -1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des donnees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection des valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().reset_index().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The total number of missing value: ', df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualition de la distribution des donnees && Detection des valeurs aberrantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Échantillonnage des données (utiliser 10% des données)\n",
    "df_sample = df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Fonction pour créer les graphiques\n",
    "def plot_variable(col):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "    sns.histplot(df_sample[col], ax=axes[0])\n",
    "    sns.boxplot(df_sample[col], ax=axes[1], showmeans=True)\n",
    "    plt.title(col)\n",
    "    plt.show()\n",
    "\n",
    "# Sélectionner les colonnes numériques\n",
    "numeric_cols = df_sample.select_dtypes('number').columns\n",
    "\n",
    "# Mesurer le temps d'exécution\n",
    "start_time = time.time()\n",
    "\n",
    "# Traitement par lots\n",
    "batch_size = 10  # Nombre de colonnes à traiter par lot\n",
    "for i in range(0, len(numeric_cols), batch_size):\n",
    "    batch_cols = numeric_cols[i:i+batch_size]\n",
    "    Parallel(n_jobs=-1)(delayed(plot_variable)(col) for col in batch_cols)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Temps d'exécution : {execution_time} secondes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_high_correlation_features(df, target_column):\n",
    "        \"\"\"\n",
    "        Retourne les colonnes corrélées à la variable cible avec un coefficient de corrélation\n",
    "        supérieur à 0.5 ou inférieur à -0.5.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        high_corr_features : list\n",
    "            Liste des noms de colonnes corrélées à la variable cible.\n",
    "        \"\"\"\n",
    "        correlation_matrix = df.corr(numeric_only=True)\n",
    "        target_corr = correlation_matrix[target_column]\n",
    "        high_corr_features = target_corr[(target_corr > 0.5) | (target_corr < -0.5)].index.tolist()\n",
    "        high_corr_features.remove(target_column)\n",
    "        return high_corr_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_high_correlation_features(df, 'Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etude explorative de la variable cible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter les occurrences de chaque catégorie\n",
    "counts = df['Label'].value_counts()\n",
    "\n",
    "# Préparer les données pour le diagramme circulaire\n",
    "labels = ['Anomalie', 'Normal']\n",
    "sizes = [counts[1], counts[0]]\n",
    "colors = ['#ff9999','#66b3ff']\n",
    "explode = (0.1, 0)\n",
    "\n",
    "# Créer le diagramme circulaire\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=140)\n",
    "\n",
    "plt.axis('equal')  # Assure que le diagramme est dessiné en cercle.\n",
    "plt.title('Exploration de la variable cible')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \"\"\"\n",
    "    Prétraitement des Données\n",
    "\n",
    "    • Nettoyage : Imputer les valeurs manquantes.\n",
    "    • Encodage des variables catégorielles : Appliquer l’encodage par variables factices (dummy variables)\n",
    "      pour les données non numériques.\n",
    "    • Standardisation : Standardiser les données numériques pour améliorer les performances des algorithmes.\n",
    "    • Livrable : Code et documentation du pipeline de prétraitement des données.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Le DataFrame contenant les données à traiter.\n",
    "    target_column : str\n",
    "        La colonne cible pour le modèle.\n",
    "    exclude_columns : list\n",
    "        Les colonnes à exclure du prétraitement (par défaut, aucune colonne n'est exclue).\n",
    "    test_size : float\n",
    "        La proportion de l'ensemble de test (par défaut, 0.2).\n",
    "    random_state : int\n",
    "        Graine aléatoire pour la reproductibilité du découpage en train/test (par défaut, 0).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, target_column, exclude_columns=None, test_size=0.2, random_state=0):\n",
    "        \n",
    "        self.df = df\n",
    "        self.target_column = target_column\n",
    "        self.exclude_columns = exclude_columns if exclude_columns is not None else []\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "\n",
    "\n",
    "    def impute_outliers(self, colname): \n",
    "        q1 = np.percentile(self.df[colname], 25)\n",
    "        q3 = np.percentile(self.df[colname], 75) \n",
    "\n",
    "        lower_bound = q1 - 1.5*(q3 - q1)\n",
    "        upper_bound = q3 + 1.5*(q3 - q1)\n",
    "\n",
    "        self.df.loc[(self.df[colname] <= lower_bound), colname] = lower_bound\n",
    "        self.df.loc[(self.df[colname] >= upper_bound), colname] = upper_bound\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Impute les valeurs aberrantes pour toutes les colonnes numériques.\n",
    "        \"\"\"\n",
    "        for colname in self.df.select_dtypes('number').columns:\n",
    "            self.impute_outliers(colname)\n",
    "        \n",
    "\n",
    "    def split_data(self):\n",
    "        \"\"\"\n",
    "        Divise les données en ensembles d'entraînement et de test.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        X_train, X_test, y_train, y_test : tuple\n",
    "            Données divisées en ensembles d'entraînement et de test.\n",
    "        \"\"\"\n",
    "        y = self.df[self.target_column]\n",
    "        X = self.df.drop([self.target_column] + self.exclude_columns, axis='columns')\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                 X, \n",
    "                                 y, \n",
    "                                 test_size=self.test_size, \n",
    "                                 random_state=self.random_state\n",
    "                            )\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    \n",
    "\n",
    "    def create_pipeline(self, X_train):\n",
    "        \"\"\"\n",
    "        Crée un pipeline de prétraitement pour les données.\n",
    "\n",
    "        Étapes du pipeline :\n",
    "        - Imputation des valeurs manquantes.\n",
    "        - Standardisation des colonnes numériques.\n",
    "        - Encodage des variables catégorielles avec des variables factices.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_train : pd.DataFrame\n",
    "            Données d'entraînement pour déterminer les types de colonnes.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        full_pipeline : ColumnTransformer\n",
    "            Pipeline de transformations pour le prétraitement complet des données.\n",
    "        \"\"\"\n",
    "        num_cols = X_train.select_dtypes(include=['number']).columns\n",
    "        cat_cols = X_train.select_dtypes(include='object').columns\n",
    "        num_pipeline = make_pipeline(\n",
    "                    SimpleImputer(strategy='median'),\n",
    "                    StandardScaler(),\n",
    "                )\n",
    "        cat_pipeline = make_pipeline(\n",
    "                    SimpleImputer(strategy='most_frequent'),\n",
    "                    OneHotEncoder(handle_unknown='ignore', drop='first')\n",
    "                )\n",
    "        full_pipeline = ColumnTransformer([\n",
    "                            ('num', num_pipeline, num_cols),\n",
    "                            ('cat', cat_pipeline, cat_cols)\n",
    "                        ])\n",
    "        return full_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = DataPreprocessor(df, 'Label',['Flow ID','Src IP','Src Port'\t,'Dst IP','Dst Port','Protocol','Timestamp'],random_state=10)\n",
    "X_train, X_test, y_train, y_test = preprocessing.split_data()\n",
    "pipeline = preprocessing.create_pipeline(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transform = pipeline.fit_transform(X_train)\n",
    "X_test_transform = pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline avec SMOTE et LightGBM\n",
    "pipeline = ImbPipeline(steps=[\n",
    "    ('equilibrage_classes', SMOTE(sampling_strategy='auto', random_state=42)),  # Gestion du déséquilibre des classes\n",
    "    ('modele', lgb.LGBMClassifier())  # Modèle LightGBM\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimisation des hyperparamètres avec GridSearchCV\n",
    "grille_parametres = {\n",
    "    'modele__num_leaves': [31, 50, 70],\n",
    "    'modele__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'modele__n_estimators': [100, 200, 300]\n",
    "}\n",
    "\n",
    "recherche_grille = GridSearchCV(pipeline, grille_parametres, cv=5, scoring='f1', verbose=2, n_jobs=-1)\n",
    "recherche_grille.fit(X_train_transform, y_train)\n",
    "\n",
    "print(\"Meilleurs paramètres GridSearchCV :\", recherche_grille.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimisation des hyperparamètres avec Optuna\n",
    "def fonction_objectif(essai):\n",
    "    parametres = {\n",
    "        'num_leaves': essai.suggest_int('num_leaves', 20, 100),\n",
    "        'learning_rate': essai.suggest_loguniform('learning_rate', 0.001, 0.1),\n",
    "        'n_estimators': essai.suggest_int('n_estimators', 50, 300)\n",
    "    }\n",
    "    modele_lightgbm = lgb.LGBMClassifier(**parametres)\n",
    "    \n",
    "    pipeline_optuna = ImbPipeline(steps=[\n",
    "        ('equilibrage_classes', SMOTE(sampling_strategy='auto', random_state=42)),  # SMOTE\n",
    "        ('modele', modele_lightgbm)  # Modèle optimisé\n",
    "    ])\n",
    "    \n",
    "    pipeline_optuna.fit(X_train_transform, y_train)\n",
    "    return pipeline_optuna.score(X_test, y_test)\n",
    "\n",
    "etude = optuna.create_study(direction='maximize')\n",
    "etude.optimize(fonction_objectif, n_trials=20)\n",
    "\n",
    "print(\"Meilleurs paramètres Optuna :\", etude.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
